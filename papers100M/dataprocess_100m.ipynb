{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b696f5-7f97-493c-9fe7-c845d943b6ad",
   "metadata": {},
   "source": [
    "For our `ogbn-papers100M` experiments, we have 256GB memory. If you have enough memory, you can simply run `run_ogbn.py` for `ogbn-papers100M`. However, due to the hugh size of the dataset, we have to preprocess some data. It is recommended to run this jupyter notebook first before you run `run_100m.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fc940-3d37-4863-8674-9a4c6278a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/chendi/ibmb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496cbdb-f765-408a-a765-c23a2af5c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afa688-bd13-4e42-8996-8d9f3714e7fa",
   "metadata": {},
   "source": [
    "## load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e105a0-34b0-44b4-b73c-4bfde362733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bf90f-94dd-4114-9b67-aa1067093524",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(name=\"ogbn-papers100M\", root='/nfs/students/qian')  # use your /path/to/data\n",
    "\n",
    "splits = dataset.get_idx_split()\n",
    "train_indices = splits['train'].numpy()\n",
    "val_indices = splits['valid'].numpy()\n",
    "test_indices = splits['test'].numpy()\n",
    "\n",
    "with open('splits.pkl', 'wb') as handle:\n",
    "    pickle.dump((train_indices, val_indices, test_indices), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c68ecd-73dd-4f8b-ad8d-3868fc08d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('splits.pkl', 'rb') as handle:\n",
    "    (train_indices, val_indices, test_indices) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f8789-5a54-4f61-84d1-1ffbfc9eb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3e936-a192-4a00-89c7-a6efa7b131cb",
   "metadata": {},
   "source": [
    "## remove some currently unneeded data to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef1870-7424-47c1-b385-d9dbf07a5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = graph.edge_index\n",
    "num_nodes = graph.num_nodes\n",
    "\n",
    "del dataset, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b54eb-4de3-4f77-bc19-7faacafc4514",
   "metadata": {},
   "source": [
    "## process adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf0892-1639-41a3-821a-97720257eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "from dataloaders.BaseLoader import BaseLoader\n",
    "\n",
    "data = torch.ones_like(row, dtype=torch.bool)\n",
    "adj = SparseTensor(row=row, col=col, value=data, sparse_sizes=(num_nodes, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37483098-6d6d-426a-9278-b90dd7e311c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj + adj.t() + SparseTensor.eye(num_nodes, dtype=torch.bool)\n",
    "adj = BaseLoader.normalize_adjmat(adj, 'sym')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9dbfe-b46c-49f3-82a1-30162fb3f01f",
   "metadata": {},
   "source": [
    "## save adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355eae84-e11c-450a-a1fa-470c0236e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "\n",
    "torch.save(adj, '/nfs/students/qian/adj.pt')  # use your /path/to/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_t = torch.load('/nfs/students/qian/adj.pt')\n",
    "\n",
    "scipy_adj = adj_t.to_scipy('csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f1675-5d90-4166-8142-689d97110bf7",
   "metadata": {},
   "source": [
    "## calculate ppr matrices for train, val and test split\n",
    "\n",
    "See https://github.com/TUM-DAML/pprgo_pytorch/blob/master/pprgo/ppr.py for method reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc38ec-9d87-4791-bfc8-efee7fa401a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.utils import get_partitions, topk_ppr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33ac3c-a823-4cc6-a11b-ed142ec82580",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 96\n",
    "alpha = 0.05\n",
    "eps = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c8f4c-f7d7-4377-83c9-315d3f213d95",
   "metadata": {},
   "source": [
    "### val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87ca23-529e-4ca2-9d79-6509f51b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.vstack((adj_t.storage.row(), adj_t.storage.col()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7d84c-eb95-4f16-b8f4-08af62d26787",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = adj_t.sizes()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afeb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ppr_mat, val_neighbors = topk_ppr_matrix(edge_index,\n",
    "                    num_nodes,\n",
    "                    alpha,\n",
    "                    eps,\n",
    "                    val_indices,\n",
    "                    topk)\n",
    "\n",
    "with open('papers100m_val_ppr.pkl', 'wb') as handle:\n",
    "    pickle.dump((val_ppr_mat, val_neighbors), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3de9a2-90c6-44bf-96c8-5c9facf5e925",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b887303-1923-4540-89d3-0307ebb42c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppr_mat, train_neighbors = topk_ppr_matrix(edge_index,\n",
    "                    num_nodes,\n",
    "                    alpha,\n",
    "                    eps,\n",
    "                    train_indices,\n",
    "                    topk)\n",
    "\n",
    "with open('papers100m_train_ppr.pkl', 'wb') as handle:\n",
    "    pickle.dump((train_ppr_mat, train_neighbors), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d3248-3eff-46d8-8b54-7772c956cfd0",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e5a81-2706-421e-b200-55c93609bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppr_mat, test_neighbors = topk_ppr_matrix(edge_index,\n",
    "                    num_nodes,\n",
    "                    alpha,\n",
    "                    eps,\n",
    "                    test_indices,\n",
    "                    topk)\n",
    "\n",
    "with open('papers100m_test_ppr.pkl', 'wb') as handle:\n",
    "    pickle.dump((test_ppr_mat, test_neighbors), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68612863-3dcf-4dbe-ae70-0a7c57a62a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('papers100m_val_ppr.pkl', 'rb') as handle:\n",
    "    val_ppr_mat, val_neighbor = pickle.load(handle)\n",
    "\n",
    "with open('papers100m_train_ppr.pkl', 'rb') as handle:\n",
    "    train_ppr_mat, train_neighbors = pickle.load(handle)\n",
    "\n",
    "with open('papers100m_test_ppr.pkl', 'rb') as handle:\n",
    "    test_ppr_mat, test_neighbors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8bdfac-0efa-4eb5-a568-c7c60f517c79",
   "metadata": {},
   "source": [
    "## Node-wise IBMB batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a906bf-bd60-4672-9f01-ec35ac3d835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output_node_per_batch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb73606-f94a-4fb3-82d0-187e34ddc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.IBMBNodeLoader import get_pairs, prime_orient_merge, prime_post_process\n",
    "\n",
    "def prime_ppr_loader(ppr_matrix, output_indices, neighbors, num_aux_per_node):\n",
    "    ppr_matrix = ppr_matrix[:, output_indices]\n",
    "    ppr_pairs = get_pairs(ppr_matrix)\n",
    "\n",
    "    output_list = prime_orient_merge(ppr_pairs, num_aux_per_node, len(output_indices))\n",
    "    output_list = prime_post_process(output_list, num_aux_per_node)\n",
    "    node_wise_out_aux_pairs = []\n",
    "\n",
    "    if isinstance(neighbors, list):\n",
    "        neighbors = np.array(neighbors, dtype=object)\n",
    "\n",
    "    _union = lambda inputs: np.unique(np.concatenate(inputs))\n",
    "    for p in output_list:\n",
    "        node_wise_out_aux_pairs.append((output_indices[p], _union(neighbors[p]).astype(np.int64)))\n",
    "    return node_wise_out_aux_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec044a-f0b4-4a81-a7ec-f15617bffc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = prime_ppr_loader(val_ppr_mat, \n",
    "                                     val_indices, \n",
    "                                     val_neighbor, \n",
    "                                     num_output_node_per_batch * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e7265-c411-4578-8d67-08890f6c80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = prime_ppr_loader(test_ppr_mat, \n",
    "                                     test_indices, \n",
    "                                     test_neighbors, \n",
    "                                     num_output_node_per_batch * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae480de-38a6-4476-88c6-625328abe589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prime_ppr_loader(train_ppr_mat, \n",
    "                                     train_indices, \n",
    "                                     train_neighbors, \n",
    "                                     num_output_node_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9cb5a-c543-4b3c-91b4-382f01bae3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('papers100m_train_ppr_batches.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('papers100m_val_ppr_batches.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('papers100m_test_ppr_batches.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cda11-1151-477d-8f43-d31b6bea23bc",
   "metadata": {},
   "source": [
    "## Batch-wise IBMB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27e807-c2ab-414e-ab0a-f9fafe879cb3",
   "metadata": {},
   "source": [
    "This is tricky for `ogbn-papers100M` dataset. \n",
    "\n",
    "Because the dataset is quite large, METIS partitioning cannot be directly applied. \n",
    "\n",
    "For each split, e.g. train split, we obtain some neighborhood of each node, and take the induced subgraph. \n",
    "\n",
    "Then we do partitioning on the subgraph.\n",
    "\n",
    "Finally, we merge the primary nodes in each partition, and auxiliary nodes are obtained from the topk PPR scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42e91-dea6-45c3-ba73-2ecbfec25ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1671d4-3fb9-4e48-8569-c2f78df4b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_ppr_loader(partitions, prime_indices, neighbor_list):\n",
    "    n = len(partitions)\n",
    "    batches = []\n",
    "    if isinstance(neighbor_list, list):\n",
    "        neighbor_list = np.array(neighbor_list, dtype=object)\n",
    "    for i in range(n):\n",
    "        intersect = np.intersect1d(partitions[i], prime_indices)\n",
    "        ind = np.in1d(prime_indices, intersect)\n",
    "        lst = list(neighbor_list[ind])\n",
    "        seconds = np.unique(np.concatenate(lst))\n",
    "        batches.append((intersect, seconds,))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c5d75-799e-4e01-ad93-6b43037a348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 5e-4\n",
    "train_parts, val_parts, test_parts = [256, 32, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6b76a-868e-4c14-911b-4c10382f26e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abba74-650d-47eb-a90e-f8cc9fbac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices, mat, neighbor, num_parts, naming in zip([train_indices, val_indices, test_indices], \n",
    "                                             [train_ppr_mat, val_ppr_mat, test_ppr_mat], \n",
    "                                             [train_neighbors, val_neighbor, test_neighbors],\n",
    "                                             [train_parts, val_parts, test_parts],\n",
    "                                             ['train', 'val', 'test']):\n",
    "    row, col, val = find(mat)\n",
    "    \n",
    "    mask = val > thresh\n",
    "    mask = np.unique(col[mask])\n",
    "    torch_mask = torch.from_numpy(mask).long()\n",
    "    \n",
    "    temp_adj_t = adj_t[torch_mask, :][:, torch_mask]\n",
    "    print(f'processed {naming} adj')\n",
    "    \n",
    "    _, partptr, perm = temp_adj_t.partition(num_parts=num_parts, recursive=False, weighted=False)\n",
    "    print(f'partitioned {naming} adj')\n",
    "    \n",
    "    partitions = []\n",
    "    for i in range(len(partptr) - 1):\n",
    "        partitions.append(mask[perm[partptr[i] : partptr[i + 1]].numpy()])\n",
    "    print(f'obtained {naming} partitions')\n",
    "    \n",
    "    batches = partition_ppr_loader(partitions, indices, neighbor)\n",
    "    print(f'obtained {naming} batches')\n",
    "    \n",
    "    with open(f'papers100m_{naming}_part_batches.pkl', 'wb') as handle:\n",
    "        pickle.dump(batches, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f'saved {naming} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d18616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7000621d-2999-4cbf-8b76-30c013cc7788",
   "metadata": {},
   "source": [
    "### visualize weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69982de-b44b-4692-8c19-cadd3f4f65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col, val = find(train_ppr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2fbde-c994-4316-af61-b6e7c3ef972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.set(xscale=\"log\")\n",
    "sns.histplot(val, ax=ax, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ab9c5-7829-44a7-bbc6-f1bf47f72f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
